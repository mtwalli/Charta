{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_text(files):\n",
    "    raw_text = \"\"\n",
    "    for file in files:\n",
    "        doc = pymupdf.open(file)\n",
    "        for page in doc:\n",
    "            raw_text += page.get_text()\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE\\nVisualization conference. The final version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/\\nFrom Data to Story: Towards Automatic Animated Data Video Creation\\nwith LLM-based Multi-Agent Systems\\nLeixian Shen\\n*\\nThe Hong Kong University\\nof Science and Technology,\\nHong Kong SAR, China\\nHaotian Li\\n†\\nThe Hong Kong University\\nof Science and Technology,\\nHong Kong SAR, China\\nYun Wang\\n‡\\nMicrosoft,\\nBeijing, China\\nHuamin Qu\\n§\\nThe Hong Kong University\\nof Science and Technology,\\nHong Kong SAR, China\\nABSTRACT\\nCreating data stories from raw data is challenging due to humans’\\nlimited attention spans and the need for specialized skills. Recent\\nadvancements in large language models (LLMs) offer great oppor-\\ntunities to develop systems with autonomous agents to streamline\\nthe data storytelling workflow. Though multi-agent systems have\\nbenefits such as fully realizing LLM potentials with decomposed\\ntasks for individual agents, designing such systems also faces chal-\\nlenges in task decomposition, performance optimization for sub-\\ntasks, and workflow design.\\nTo better understand these issues,\\nwe develop Data Director, an LLM-based multi-agent system de-\\nsigned to automate the creation of animated data videos, a repre-\\nsentative genre of data stories. Data Director interprets raw data,\\nbreaks down tasks, designs agent roles to make informed deci-\\nsions automatically, and seamlessly integrates diverse components\\nof data videos. A case study demonstrates Data Director’s effec-\\ntiveness in generating data videos. Throughout development, we\\nhave derived lessons learned from addressing challenges, guiding\\nfurther advancements in autonomous agents for data storytelling.\\nWe also shed light on future directions for global optimization,\\nhuman-in-the-loop design, and the application of advanced multi-\\nmodal LLMs.\\nIndex Terms: Data Storytelling, LLM, Multi-Agent, Data Video\\n1\\nINTRODUCTION\\nThe rapid growth of data assets has driven advancements in various\\ndomains, but it has also presented challenges for human-data inter-\\naction. Humans have limited attention spans and may lack the spe-\\ncialized skills to extract valuable insights and craft engaging data\\nstories across multiple modalities [11]. Automating the generation\\nof stories from raw data can greatly enhance the efficiency of data\\nanalysis and information communication.\\nRecently, advancements in large language models (LLMs) have\\nshowcased robust natural language understanding and reasoning ca-\\npabilities, proving effective across various tasks like data analy-\\nsis [34, 4], document generation [14], and visualization creation [7].\\nThese capabilities open up new avenues to streamline the entire\\ndata storytelling workflow by developing systems featuring LLM-\\npowered autonomous agents. In this paradigm, LLMs serve as the\\ncognitive core of these agents, enabling them to perceive environ-\\nments (Perception), make decisions (Brain), and take responsive\\nactions (Action), thereby assisting humans in automating a wide\\nrange of tasks [35].\\nTherefore, we aim to explore the potential of LLM-based au-\\ntonomous agents in facilitating end-to-end storytelling directly\\n*e-mail: lshenaj@connect.ust.hk\\n†e-mail: haotian.li@connect.ust.hk\\n‡e-mail: wangyun@microsoft.com\\n§e-mail: huamin@cse.ust.hk\\nFigure 1: Architecture of Data Director.\\nfrom raw data, which is a new problem in the visualization and sto-\\nrytelling community. In this paper, we specifically focus on a rep-\\nresentative genre of data stories [19], animated data videos, which\\nencompass diverse components and necessitate the coordination of\\nthese diverse elements [6]. Existing automatic methods for creating\\ndata videos either require users to prepare various materials from\\nraw data [25, 32, 26] or still involve complex and time-consuming\\nmanual authoring processes [5, 30, 8, 27, 3]. We envision that au-\\ntonomous agents can facilitate the automatic transformation of raw\\ndata into animated data videos. However, achieving this goal in-\\nvolves overcoming several challenges:\\n• Task Decomposition: Data storytelling involves the generation\\nand coordination of diverse elements such as visualizations, text\\nnarrations, audio, and animations. The system should accurately\\ninterpret raw data, break down the storytelling task into manage-\\nable sub-tasks, and assign appropriate roles to agents specialized\\nin handling specific aspects of the task.\\n• Performance Optimization: In each sub-task, the agent is re-\\nquired to make informed decisions based on perception inputs\\nand determine the appropriate tools and methods to use. Each\\nsub-task often relies on the outputs of preceding stages, high-\\nlighting the interdependence among these sub-tasks. So ensuring\\noptimal performance for each one is crucial.\\n• Workflow Design:\\nStorytelling involves numerous intercon-\\nnected sub-tasks with diverse sequence schemes. Tasks such as\\ndata visualization, crafting narration, recording audio, design-\\ning animations, and aligning diverse components are typically\\nnon-linear and their order may vary, presenting challenges in de-\\ntermining the optimal approach. The system needs to facilitate a\\nseamless workflow that automates and effectively integrates all\\nthese sub-tasks.\\nTo better understand these issues, we develop Data Director, an\\n1\\narXiv:2408.03876v1  [cs.HC]  7 Aug 2024\\n© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization\\nconference. The final version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/\\nLLM-based multi-agent system that automates the entire process of\\ntransforming raw data into engaging animated data videos. The sys-\\ntem’s architecture is shown in Fig. 1. Specifically, we decompose\\ndata video creation into distinct sub-tasks based on data video com-\\nponents and their relationships. We design two agent roles—data\\nanalyst and designer—to manage and conduct these sub-tasks, and\\noptimize the performance of each sub-task through prompt design.\\nWe also explore effective ways to interconnect these sub-tasks and\\niteratively refine the workflow. To demonstrate the effectiveness of\\nData Director, we conduct a case study where Data Director gen-\\nerates a data video about real-world stock price data. Finally, we\\nsummarize the lessons learned from our system design for task de-\\ncomposition, performance optimization, and workflow design, and\\nprovide insights to guide future developments in multi-agent sys-\\ntems for the automatic transformation of raw data into data stories.\\n2\\nDATA DIRECTOR\\nThis section will first give an overview of Data Director’s architec-\\nture and then introduce our design practices.\\n2.1\\nOverview\\nData Director is an LLM-based multi-agent system powered by\\nGPT-4 [2].\\nWe follow existing conceptual framework of LLM-\\nbased agent to design three components: perception, brain, and ac-\\ntion [35]. The architecture of Data Director is illustrated in Fig. 1,\\nwith a central controller (a) scheduling all components.\\nUser-\\ngenerated data (b) is directly input into Data Director. The percep-\\ntion module (c) preprocesses the data, which is then fed into the first\\nagent acting as a data analyst (d). This agent’s tasks include extract-\\ning insights, visualizing data, and crafting narration text. The gen-\\nerated visualization and narration text are passed to the next agent,\\nwhich acts as a designer (e). This agent is responsible for animating\\nand annotating the content, as well as coordinating the data video\\ncomponents. Finally, the controller (a) utilizes the decisions made\\nby the multi-agent system to generate a data video with relevant\\ntools (f).\\n2.2\\nTask Decomposition and Workflow Design\\nTo design multi-agent systems for complex tasks, such as data sto-\\nrytelling, it is crucial to decompose the process into patterned sub-\\ntasks, facilitating model learning and interpretation [35]. Task de-\\ncomposition is a balance art between accuracy and efficiency.\\nCoarse tasks may exceed the model’s capabilities, leading to hal-\\nlucinations or non-computational results, while excessively fine-\\ngrained tasks may overwhelm the model with excessive tasks, af-\\nfecting efficiency and increasing costs.\\nTo decompose the tasks in the context of data video creation,\\nwe first break down data videos into basic components and the\\nrelationship between these components, following previous re-\\nsearch [32, 25]. The basic components of data videos include data\\nvisualizations, text narrations, and visual animations. To make sure\\nthese components appear coherently in data videos, three relation-\\nships need to be taken care of: 1) Animated visualization elements\\nmust semantically connect with corresponding text narration seg-\\nments; 2) Animation effects must be tailored to the visualization el-\\nements they accompany; 3) Text narration should align temporally\\nwith the animations, serving as the timeline.\\nWith these identified components and relationships, we assign\\nthe three component creation tasks and the three relationship man-\\nagement tasks (six sub-tasks in total) to two LLM-powered agents,\\nassuming the roles of a data analyst and a designer. Additionally,\\nwe develop a controller to manage the input and output of each\\nmodule, parse outputs, and invoke appropriate tools for tasks be-\\nyond LLM capabilities.\\nFig. 2 illustrates a case study based on real-world stock price\\ndata of four IT companies.\\nPerception. The perception module accepts and processes diverse\\ninformation from external environments, transforming it into un-\\nderstandable representations for LLMs [35]. In Data Director, data\\ntables are inputted directly in the prompt as formatted text. During\\nthe perception phase (A), a data preprocessing module utilizes the\\ndataset’s title and content within an LLM session to generate nat-\\nural language descriptions. As shown in Fig. 2-A, the description\\ninvolves a high-level overview of the data topic, along with a de-\\ntailed elaboration of the semantics of each data column, enhancing\\ncontextual information for subsequent model processes. The gener-\\nated NL data description and the raw data are then fed into the data\\nanalysis brain.\\nRole as Data Analyst.\\nInspired by insight-based visualization\\nand understanding techniques [39, 33], when analyzing data, Data\\nDirector first prompts the LLM to extract top-k interesting data\\ninsights (B) from raw data, based on data analysis task model-\\ning [23, 21]. These insights guide the generation of visualizations\\n(sub-task 1) and text narration (sub-task 2). Following the Chain-\\nof-Thought strategy [9] and carefully designed LLM prompts, the\\nbrain incrementally derives a list of insights (B), declarative Vega-\\nLite visualizations [18] (C), and text narrations (D) step by step.\\nRole as Designer. Once the “data analyst” has prepared visual-\\nizations and corresponding text narrations, the “designer” agent fo-\\ncuses on creating dynamic animations (sub-task 3) and synchroniz-\\ning components (sub-task 4 5 6). Animations are categorized into\\ntwo types: visual effects (e.g., fade, grow, fly, zoom, etc.) applied\\nto visualization elements, and annotations that introduce additional\\nvisual elements at the right moments. Animation effects are de-\\ntermined by distilling choices from an animation library into nat-\\nural language prompts (E). The agent is tasked with selecting the\\noptimal timing for animation applications, identifying the precise\\nvisual elements that will be animated, and choosing the appropri-\\nate animation effects. Furthermore, given the complexity of Vega-\\nLite-specified annotated visualizations, we adopt a hierarchical ap-\\nproach, initially generating base visualizations with the data ana-\\nlyst agent and subsequently enhancing them with annotations (with\\nVega-Lite specifications) for improved outcomes (F) using the de-\\nsigner agent. Text narration serves as a timeline, transforming tem-\\nporal synchronization into semantic links between static narration\\nsegments and visual elements [32, 25].\\nController. As shown in the middle of Fig. 2, based on the NL\\noutputs of the models, the controller first utilizes text-to-speech ser-\\nvices to convert text narrations into audio with precise timestamps,\\nensuring alignment with the data video timeline. Then, the con-\\ntroller invokes a visualization renderer to convert Vega-Lite spec-\\nifications into SVG files. Each SVG element is automatically as-\\nsociated with blackened data and visualization structure informa-\\ntion [25, 28]. Narration segments are further linked to these SVG\\nvisual elements based on the text-visual links established by the\\ndesigner agent, akin to Data Player [25]. Then the corresponding\\nanimation effects are applied to the SVG elements based on the de-\\nsigner agent’s decisions. Next, annotated visualizations are parsed\\nto detect SVG annotation elements, integrating “fade in” anima-\\ntions at corresponding timestamps. Finally, the controller calls a\\nvideo synthesizer to merge visualizations, audio narration, and ani-\\nmation sequences into the final data video.\\n3\\nCASE STUDY\\nFig. 2 illustrates a real-world case study using stock price data. Data\\nDirector generates diverse insights (B) and visualizes stock prices\\nover time using line charts (C). The narration followed a structured\\napproach (C): starting with an overview, detailing each company’s\\nperformance, and concluding with a summary. Based on the nar-\\nration, points and text annotations were added to each company’s\\nline to highlight its notable characteristic (F). The bottom of Fig. 2\\ndisplays key animation frames (E) from the video, with each frame\\n2\\n© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization\\nconference. The final version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/\\nFigure 2: An example walkthrough of Data Director.\\nnumbered to correspond with specific timestamps in the narration\\n(D). This demonstrates when specific animations are triggered. The\\nfinal data video starts with an entrance animation showcasing all\\nelements, and when discussing each company, the respective line\\nis highlighted individually. Overall, the entire narrative is well-\\ncrafted, with smooth articulation and appropriately designed visu-\\nalizations and animations, resulting in an engaging data video.\\n4\\nLESSONS LEARNED\\nThis section will discuss the lessons learned throughout the devel-\\nopment of Data Director, focusing on task decomposition, perfor-\\nmance optimization, and workflow design.\\n4.1\\nTask Decomposition\\nBalancing Accuracy and Efficiency. Task decomposition for data\\nstorytelling requires balancing accuracy and efficiency. When de-\\ncomposing tasks, first, it is essential to identify which tasks the\\nmodel excels in (e.g., natural language generation, reasoning, and\\ntext-based decision-making) and distinguish these from tasks that\\nnecessitate external tools (e.g., visualization rendering, audio gen-\\neration, and video synthesizing).\\nSecond, the sub-tasks result-\\ning from decomposition should be well-defined and manageable.\\nThese sub-tasks can then be grouped to shape agent roles that\\nalign with the inherent characteristics of the tasks. For example,\\nData Director breaks down tasks based on the data video compo-\\n3\\n© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization\\nconference. The final version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/\\nnents, and organizes sub-tasks into analysis-focused tasks that gen-\\nerate static content from raw data, and design-focused tasks that\\nrequire creative input and the derivation of dynamic effects from\\nthe static content. Third, the suitable combination of sub-tasks can\\nenhance both the model’s accuracy and efficiency, as demonstrated\\nin Sec. 2.2, where animation design and temporal synchronization\\nwere merged. Finally, a top-down approach can be adopted to dis-\\nsect tasks progressively, designing suitable and efficient models for\\neach sub-task.\\nData Feeding with Contextual Information. Providing the model\\nwith ample contextual information has been found to enhance the\\naccuracy and quality of its generation [34]. As the model progresses\\nthrough sub-tasks step by step, the context is enriched and updated,\\noffering more information for subsequent operations. For example,\\nin the data analyst agent, the model gradually gathers information\\non data descriptions, insights, visualizations, and narrations. How-\\never, when an agent perceives data from the environment, the data\\nitself is merely numerical with limited context. We have found that\\nsemantically enriching the data and supplying the model with con-\\ntextual insights significantly enhance its effectiveness. For exam-\\nple, Data Director uses the LLM to generate an NL description of\\na data table with a title. Future research could integrate innovative\\ntechniques for enhancing data comprehension and exploring novel\\nmethods for inputting data into LLMs.\\n4.2\\nPerformance Optimization\\nEffective prompt design is crucial for enhancing the performance\\nand output quality of LLM-based agent systems. The complete\\nprompt of Data Director can be found in Appendix A. The key\\nstrategies for optimizing prompts within this context are outlined\\nas follows:\\nAssignment of Appropriate Tasks for LLMs. The foundation\\nof effective prompt design lies in the careful design of tasks for\\nLLMs. It is essential to identify the tasks where LLMs are good\\nat. Furthermore, assigning the model a specific role, such as a data\\nanalyst or designer in Data Director, can guide the model to pro-\\nduce domain-specific and contextually relevant outputs. In addi-\\ntion, supplying the model with precise and comprehensive context\\ncan enhance its understanding of tasks, thereby improving the ac-\\ncuracy and relevance of its responses. This involves crafting well-\\nstructured prompts (outlined in Appendix A) and designing com-\\nplementary modules like data preprocessing in Data Director.\\nCognitive Processing Time and Task Decomposition.\\nAllow-\\ning the model adequate cognitive processing time is essential for\\nachieving high-quality outputs and alleviating hallucinations. Be-\\nyond the task decomposition discussed above, in terms of prompt\\ndesign, the sub-tasks can be clearly defined with sequential steps,\\nfacilitating the application of the Chain-of-Thought strategy. More-\\nover, the number of tasks within one prompt should be balanced to\\ndeduce task difficulty. In addition, prompting the model to explain\\nits decisions or outline its solution methodology before concluding\\ncan promote a more thoughtful and precise response generation pro-\\ncess. For instance, in the designer agent, Data Director prompts the\\nLLM to its choices regarding the animation and annotation design,\\nwhich enhances the decision accuracy and simplifies the debugging\\nof LLM applications during development.\\nCrafting Precise and Unambiguous Instructions. The clarity and\\nspecificity of instructions are paramount in effective prompt design.\\nUtilizing delimiters (e.g., “‘ “‘, “ ” or <>) to segment prompt sec-\\ntions can reduce ambiguity and aid comprehension. Providing fine-\\ngrained requirements and employing the correct use of keywords\\n(e.g., “summarize” vs. “extract”) ensures that the model adheres\\nclosely to the task parameters. Furthermore, requesting structured\\noutputs (e.g., JSON and HTML) and offering a range of response\\noptions can guide the model toward producing organized and prac-\\ntical outputs. For example, as shown in Fig. 2, Data Director allows\\nLLMs to select insight types (B), animation types (E), and anno-\\ntation types (F) from a set of predefined candidates. Additionally,\\nincorporating conditional logic (e.g., if-else statements), employing\\nfew-shot or one-shot prompting techniques with curated examples,\\nand referencing URLs for concrete examples can further enhance\\nthe model’s understanding and task execution accuracy. More spe-\\ncific examples can be found in Appendix A.\\n4.3\\nWorkflow Design\\nShared Representation. This paper primarily utilizes the GPT-4\\nmodel [2], with natural language serving as the medium for input\\nand output, setting the stage for this discussion. Effective commu-\\nnication between agents and between agents and external tools re-\\nquires an appropriate shared NL representation. For instance, Vega-\\nLite is employed in Data Director to represent all visualizations and\\nannotations, while insights and animated visual information are en-\\ncapsulated in a JSON format, incorporating specific feature infor-\\nmation (see Fig. 2). Such shared representations must be compre-\\nhensible and easily generated by the model and readily interpreted\\nby external tools for mapping to internal operations. Future work\\ncould involve designing a global shared representation for specific\\napplication scenarios to assist models in better preserving and gen-\\nerating contextual information.\\nIterative Development.\\nVarious sub-tasks within the workflow\\npresent diverse sequencing strategies. For instance, annotations can\\nbe generated simultaneously with visualizations, during animation\\ngeneration, or using a hierarchical approach as described in Data\\nDirector. Similarly, in data analysis, one may opt to produce vi-\\nsualizations either before or after narration. Designing the optimal\\nsequencing strategy is challenging due to the absence of a quantita-\\ntive global optimization objective. Hence, for applications of LLM-\\nbased agents, we adopt an iterative design methodology based on\\ntask decomposition and local performance optimization [1]. This\\ninvolves a cycle of ideation, implementation, experimental evalu-\\nation, and error analysis. Striving for consistency in the model’s\\noutputs also necessitates adherence to established guidelines and\\nmeticulous parameter adjustments. We note that Data Director pre-\\nsented here is the result of our iterative optimizations and may not\\nnecessarily represent the optimal configuration. Our intention in\\ndeveloping this prototype tool is to uncover valuable lessons and\\ninsights that can guide future research.\\n5\\nFUTURE WORK\\nGlobal Optimization and Benchmarking. The iterative develop-\\nment of multi-agent systems, as mentioned in Sec. 2.2, suffers from\\nthe lack of a global optimization and validation framework for end-\\nto-end data video generation [16]. Additionally, the community\\nlacks a widely recognized benchmark. The complexity of this chal-\\nlenge is compounded by the inherently subjective nature of data\\nstorytelling quality, which is subject to individual interpretation\\nand the multifaceted decision-making involved in various narrative\\nforms. Future work could include summarizing relevant rubrics and\\nconducting empirical studies to derive quantitative guidelines. With\\nthese well-defined metrics, an evaluation agent can also be added to\\nenhance existing workflow. Additionally, there is a need to develop\\na universally shared representation for optimization and incorpo-\\nrate domain-specific languages and objectives tailored to diverse\\nscenarios [15, 17].\\nHuman-in-the-Loop. Data-driven end-to-end generation solutions\\ncan result in one-size-fits-all outputs. To address the issues, in-\\ncorporating human-in-the-loop is an essential approach to com-\\npensate for model limitations and generate more personalized re-\\nsults [11, 10]. In data storytelling, three paradigms of human-in-\\nthe-loop can be further explored: firstly, allowing users to input\\nmore information in the perception module while maintaining the\\ncurrent architecture, articulating their goals and requirements in the\\n4\\n© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization\\nconference. The final version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/\\nforms like natural language [20], example [36, 22], and sketch [14];\\nsecondly, integrating humans into sub-tasks to achieve local opti-\\nmization before proceeding to the next stage, such as generating\\nmultiple candidates for visualization and annotation after generat-\\ning data insights; thirdly, users providing conversational feedback\\nbased on the output [20, 31], with the agent generating new end-to-\\nend results based on this feedback. Additionally, these methods can\\nalso be flexibly combined.\\nKeeping Up with Cutting-Edge Models. This paper primarily\\nuses the GPT-4 model. However, with the rapid evolution of large\\nlanguage models (LLMs), GPT-4 is swiftly being augmented by\\nthe emergence of multimodal LLMs [38]. These advanced models\\noffer expanded functionalities for handling multimodal inputs and\\noutputs, significantly impacting task decomposition, performance\\noptimization, and workflow design within our established frame-\\nwork (Fig. 1). For instance, initial generation of visualization files\\ncould be followed by refinement in a subsequent multimodal mod-\\nule, potentially leading to direct generation of video content. The\\nenhancement of model capabilities presents numerous opportuni-\\nties. Future work should not only track the latest models to develop\\nmore powerful agents but also leverage diverse models with differ-\\nent capabilities to enrich data storytelling. This includes expanding\\nbeyond individual static charts to incorporate visual and musical\\ncontent [29], supporting more complex insights and multi-view vi-\\nsualizations [13], integrating existing computational design spaces\\n(e.g., camera [12] and narrative structure [37]), and accommodating\\nmore data types (e.g., unstructured graphs [24]). Achieving these\\nfeatures requires enhancing shared representations and designing\\ncorresponding prompts (similar to how Data Director integrates an-\\nimation).\\nInherent Limitations of Large Language Models.\\nLLMs are\\npowerful but exhibit several inherent limitations, such as error accu-\\nmulation, inconsistent results, hallucinations, and high time costs.\\nMost importantly, we need to acknowledge that the content from\\nLLMs is generative but not truthful. To address error accumulation,\\nincorporating a human-in-the-loop approach and providing timely\\ntips can improve accuracy. Consistency in results can be improved\\nby strictly following established guidelines and creating supple-\\nmentary rules to handle the model’s output. Hallucinations, where\\nthe model generates plausible but incorrect information, can be im-\\nproved by implementing some prompt optimization strategies, such\\nas self-repair mechanisms, the Chain-of-Thought (CoT) approach,\\nand code-interpreter functionalities [34]. Lastly, high-time costs\\ncan be managed by breaking down tasks, finding suitable solutions\\nfor each (e.g., heuristics, basic models, and LLMs). It’s important to\\nrecognize that LLMs are not a one-size-fits-all solution; sometimes,\\nbasic models or heuristic rules can be highly effective without the\\nneed for LLMs.\\n6\\nCONCLUSION\\nThe rapid evolution of LLMs presents new opportunities for creat-\\ning end-to-end multi-agent systems for data storytelling. Through\\nthe development of Data Director, we have derived valuable in-\\nsights into task decomposition, local performance optimization\\nthrough prompt design, and workflow design.\\nIn addition, we\\nalso shed light on future directions in the development of glob-\\nally optimized multi-agents, human-in-the-loop systems, integra-\\ntion of cutting-edge multimodal models, and addressing inherent\\nLLM limitations.\\nACKNOWLEDGMENTS\\nThe authors wish to thank all reviewers for their valuable feed-\\nback. This work has been partially supported by RGC GRF Grant\\n16210321.\\nREFERENCES\\n[1] Chatgpt prompt engineering for developers.\\nhttps://learn.\\ndeeplearning.ai/chatgpt-prompt-eng/. 4\\n[2] Openai gpt-4. https://openai.com/gpt-4. 2, 4\\n[3] F. Amini, N. H. Riche, B. Lee, A. Monroy-Hernandez, and P. Irani.\\nAuthoring data-driven videos with dataclips. IEEE Transactions on\\nVisualization and Computer Graphics, 23(1):501–510, 2017. 1\\n[4] C. Beasley and A. Abouzied. Pipe(line) Dreams: Fully Automated\\nEnd-to-End Analysis and Visualization. In Proceedings of the 2024\\nWorkshop on Human-In-the-Loop Data Analytics, pp. 1–7. ACM,\\n2024. 1\\n[5] Y. Cao, J. L. E, Z. Chen, and H. Xia. DataParticles: Block-based\\nand Language-oriented Authoring of Animated Unit Visualizations.\\nIn Proceedings of the 2023 CHI Conference on Human Factors in\\nComputing Systems, CHI ’23, pp. 1–15. ACM, 2023. 1\\n[6] H. Cheng, J. Wang, Y. Wang, B. Lee, H. Zhang, and D. Zhang. In-\\nvestigating the role and interplay of narrations and animations in data\\nvideos. Computer Graphics Forum, 41(3):527–539, 2022. 1\\n[7] V. Dibia.\\nLIDA: A Tool for Automatic Generation of Grammar-\\nAgnostic Visualizations and Infographics using Large Language Mod-\\nels. In Proceedings of the 61st Annual Meeting of the Association for\\nComputational Linguistics, ACL’23, pp. 113–126. ACL, 2023. 1\\n[8] T. Ge, B. Lee, and Y. Wang. CAST: Authoring Data-Driven Chart\\nAnimations. In Proceedings of the 2021 CHI Conference on Human\\nFactors in Computing Systems, CHI ’21, pp. 1–15. ACM, 2021. 1\\n[9] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large\\nlanguage models are zero-shot reasoners. In S. Koyejo, S. Mohamed,\\nA. Agarwal, D. Belgrave, K. Cho, and A. Oh, eds., Proceedings of Ad-\\nvances in Neural Information Processing Systems, NIPS’22, vol. 35,\\npp. 22199–22213, 2022. 2\\n[10] H. Li, Y. Wang, Q. V. Liao, and H. Qu. Why is AI not a Panacea for\\nData Workers? An Interview Study on Human-AI Collaboration in\\nData Storytelling. arXiv preprint arXiv:2304.08366, 2023. 4\\n[11] H. Li, Y. Wang, and H. Qu. Where Are We So Far? Understand-\\ning Data Storytelling Tools from the Perspective of Human-AI Col-\\nlaboration. In Proceedings of CHI Conference on Human Factors in\\nComputing Systems, CHI’24, pp. 1–28, 2024. 1, 4\\n[12] W. Li, Z. Wang, Y. Wang, D. Weng, L. Xie, S. Chen, H. Zhang, and\\nH. Qu. GeoCamera: Telling Stories in Geographic Visualizations with\\nCamera Movements. In Proceedings of CHI Conference on Human\\nFactors in Computing Systems, CHI’23, pp. 1–15. ACM, 2023. 5\\n[13] Y. Lin, H. Li, A. Wu, Y. Wang, and H. Qu. DMiner: Dashboard Design\\nMining and Recommendation. IEEE Transactions on Visualization\\nand Computer Graphics, 30(7):4108–4121, 2024. 5\\n[14] Y. Lin, H. Li, L. Yang, A. Wu, and H. Qu.\\nInkSight: Leverag-\\ning Sketch Interaction for Documenting Chart Findings in Computa-\\ntional Notebooks. IEEE Transactions on Visualization and Computer\\nGraphics, 30(1):944 – 954, 2024. 1, 5\\n[15] Y. Ouyang, L. Shen, Y. Wang, and Q. Li.\\nNotePlayer: Engaging\\nJupyter Notebooks for Dynamic Presentation of Analytical Processes.\\narXiv preprint arXiv:2408.01101, pp. 1–15, 2024. 4\\n[16] J. S. Park, J. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S.\\nBernstein. Generative Agents: Interactive Simulacra of Human Be-\\nhavior. In Proceedings of the Annual ACM Symposium on User Inter-\\nface Software and Technology, UIST’23, pp. 1–22. ACM, 2023. 4\\n[17] S. Sallam, Y. Sakamoto, J. Leboe-McGowan, C. Latulipe, and P. Irani.\\nTowards Design Guidelines for Effective Health-Related Data Videos:\\nAn Empirical Investigation of Affect, Personality, and Video Content.\\nIn Proceedings of CHI Conference on Human Factors in Computing\\nSystems, CHI’22, pp. 1–22. ACM, 2022. 4\\n[18] A. Satyanarayan, D. Moritz, K. Wongsuphasawat, and J. Heer. Vega-\\nLite: A Grammar of Interactive Graphics. IEEE Transactions on Vi-\\nsualization and Computer Graphics, 23(1):341–350, 2017. 2\\n[19] E. Segel and J. Heer.\\nNarrative visualization: Telling stories with\\ndata. IEEE Transactions on Visualization and Computer Graphics,\\n16(6):1139–1148, 2010. 1\\n[20] L. Shen, E. Shen, Y. Luo, X. Yang, X. Hu, X. Zhang, Z. Tai, and\\nJ. Wang. Towards Natural Language Interfaces for Data Visualiza-\\ntion: A Survey. IEEE Transactions on Visualization and Computer\\nGraphics, 29(6):3121–3144, 2023. 5\\n5\\n© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization\\nconference. The final version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/\\n[21] L. Shen, E. Shen, Z. Tai, Y. Song, and J. Wang. TaskVis: Task-oriented\\nVisualization Recommendation.\\nIn Proceedings of the 23th Euro-\\ngraphics Conference on Visualization (Short Papers), EuroVis’21, pp.\\n91–95. Eurographics, 2021. 2\\n[22] L. Shen, E. Shen, Z. Tai, Y. Wang, Y. Luo, and J. Wang. GALVIS: Vi-\\nsualization Construction through Example-Powered Declarative Pro-\\ngramming.\\nIn Proceedings of the 31st ACM International Confer-\\nence on Information & Knowledge Management, CIKM’22, pp. 4975–\\n4979. ACM, 2022. 5\\n[23] L. Shen, E. Shen, Z. Tai, Y. Xu, J. Dong, and J. Wang. Visual Data\\nAnalysis with Task-Based Recommendations. Data Science and En-\\ngineering, 7(4):354–369, 2022. 2\\n[24] L. Shen, Z. Tai, E. Shen, and J. Wang.\\nGraph Exploration With\\nEmbedding-Guided Layouts. IEEE Transactions on Visualization and\\nComputer Graphics, 30(7):3693–3708, 2024. 5\\n[25] L. Shen, Y. Zhang, H. Zhang, and Y. Wang. Data Player: Automatic\\nGeneration of Data Videos with Narration-Animation Interplay. IEEE\\nTransactions on Visualization and Computer Graphics, 30(1):109–\\n119, 2024. 1, 2\\n[26] D. Shi, F. Sun, X. Xu, X. Lan, D. Gotz, and N. Cao. AutoClips: An\\nAutomatic Approach to Video Generation from Data Facts. Computer\\nGraphics Forum, 40(3):495–505, 2021. 1\\n[27] M. Shin, J. Kim, Y. Han, L. Xie, M. Whitelaw, B. C. Kwon, S. Ko,\\nand N. Elmqvist. Roslingifier: Semi-Automated Storytelling for Ani-\\nmated Scatterplots. IEEE Transactions on Visualization and Computer\\nGraphics, 29(6):2980–2995, 2023. 1\\n[28] L. S. Snyder and J. Heer. DIVI: Dynamically Interactive Visualiza-\\ntion. IEEE Transactions on Visualization and Computer Graphics,\\n30(1):403–413, 2024. 2\\n[29] T. Tang, J. Tang, J. Lai, L. Ying, Y. Wu, L. Yu, and P. Ren. SmartShots:\\nAn Optimization Approach for Generating Videos with Data Visual-\\nizations Embedded. ACM Transactions on Interactive Intelligent Sys-\\ntems, 12(1):1–21, 2022. 5\\n[30] J. R. Thompson, Z. Liu, and J. Stasko. Data Animator: Authoring\\nExpressive Animated Data Graphics. In Proceedings of the 2021 CHI\\nConference on Human Factors in Computing Systems, CHI’21, pp.\\n1–18. ACM, 2021. 1\\n[31] Y. Wang, Z. Hou, L. Shen, T. Wu, J. Wang, H. Huang, H. Zhang,\\nand D. Zhang. Towards Natural Language-Based Visualization Au-\\nthoring. IEEE Transactions on Visualization and Computer Graphics,\\n29(1):1222 – 1232, 2023. 5\\n[32] Y. Wang, L. Shen, Z. You, X. Shu, B. Lee, J. Thompson, H. Zhang, and\\nD. Zhang. WonderFlow: Narration-Centric Design of Animated Data\\nVideos. IEEE Transactions on Visualization and Computer Graphics,\\npp. 1–15, 2024. 1, 2\\n[33] Y. Wang, Z. Sun, H. Zhang, W. Cui, K. Xu, X. Ma, and D. Zhang.\\nDataShot:\\nAutomatic Generation of Fact Sheets from Tabular\\nData. IEEE Transactions on Visualization and Computer Graphics,\\n26(1):895–905, 2020. 2\\n[34] Y. Wu, Y. Wan, H. Zhang, Y. Sui, W. Wei, W. Zhao, G. Xu, and H. Jin.\\nAutomated Data Visualization from Natural Language via Large Lan-\\nguage Models: An Exploratory Study. In Proceedings of the ACM on\\nManagement of Data, SIGMOD’24, pp. 1–28. ACM, 2024. 1, 4, 5\\n[35] Z. Xi, W. Chen, X. Guo, W. He, and et al. The Rise and Potential of\\nLarge Language Model Based Agents: A Survey. arXiv: 2309.07864,\\npp. 1–86, 2023. 1, 2\\n[36] L. Xie, Z. Zhou, K. Yu, Y. Wang, H. Qu, and S. Chen. Wakey-Wakey:\\nAnimate Text by Mimicking Characters in a GIF.\\nIn Proceedings\\nof the 36th Annual ACM Symposium on User Interface Software and\\nTechnology, UIST’23, pp. 1–14. ACM, 2023. 5\\n[37] L. Yang, X. Xu, X. Y. Lan, Z. Liu, S. Guo, Y. Shi, H. Qu, and N. Cao.\\nA Design Space for Applying the Freytag’s Pyramid Structure to Data\\nStories. IEEE Transactions on Visualization and Computer Graphics,\\n28(1):922–932, 2022. 5\\n[38] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen. A survey\\non multimodal large language models. arXiv:2306.13549, 2023. 5\\n[39] L. Ying, Y. Wang, H. Li, S. Dou, H. Zhang, X. Jiang, H. Qu, and\\nY. Wu. Reviving Static Charts into Live Charts. IEEE Transactions\\non Visualization and Computer Graphics, pp. 1–15, 2024. 2\\n6\\n© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization\\nconference. The final version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/\\nA\\nFULL PROMPT\\nListing 1: The prompt of generating text description for data tables\\nGive a short and consistent description of the following data table and columns:\\n{{table}}\\nThe title of the data table is: {{title}}\\nThe output JSON format is:\\n{\\n\"Description\": [A]\\n}\\nwhere [A] is the generated description.\\nListing 2: The prompt of the agent acting as a data analyst\\nYou are a data analyst. You have a data table at hand.\\n{{description}}\\nThe full data table is:\\n{{table}}\\nYou need to complete several tasks, please think step by step.\\nTask 1: Please list the top insights you can gather from the following data table.\\nNotes for insight extraction:\\n- The output JSON format is:\\n[\\n{\\n\"insight\": insight content ,\\n\"type\": a list of corresponding insight types\\n}\\n]\\n- The insight type belongs to the following list: [Change Over Time, Characterize Distribution , Cluster , Comparison ,\\nCorrelate , Determine Range, Deviation , Find Anomalies , Find Extremum , Magnitude , Part to Whole, Sort, Trend]. One\\ninsight can correspond to multiple types.\\n- The selected insights should be obvious , valuable , and diverse.\\n- Double-check that the comparison of numerical magnitudes is accurate. Ensure that the insight is right.\\n- Ignore the \"index\" column.\\nTask 2: Please draw a Vega-Lite visualization to tell the insights based on the data table.\\nNotes for visualization generation:\\n- Your title should be less than 10 words.\\n- If you use the color channel , refer to the following color scheme: https://vega.github.io/vega/docs/schemes/\\n- Your visualization should have the right height and width.\\n- If the visualization is a line chart, it should include data points.\\n- If the focus of the data table presentation is on percentage information of one single column , then use a pie chart to\\npresent it. The percentage of each sector should be displayed on the corresponding sector via text annotation like\\nhttps://vega.github.io/vega-lite/examples/layer_arc_label.html.\\n- Use a single chart to visualize the data, and the index column should not be visualized.\\nTask 3: Please write a streamlined narration for the insights.\\nNotes for narration generation:\\n- Writing narration in the tone of describing the visualization instead of describing the data table.\\n- Your logic should be compelling , linking insights into a story, and avoiding enumerating insights.\\n- Avoid using additional explanations. Avoid speculating on conclusions and redundant explanations beyond the data.\\n- Content should be streamlined.\\n- Ignore the \"index\" column.\\nThe final output JSON format is:\\n{\\n\"Insights\": [A],\\n\"Visualization\": [B],\\n\"Visualization_Type\":[C]\\n\"Narration\": [D]\\n}\\nwhere [A] is the listed insights in Task 1,\\n[B] is the visualization specification generated in Task 2, [C] is the\\nvisualization type, which is one of the values of [bar, scatter , pie, line], and [D] is the narration text for the\\ninsights in Task 3.\\n7\\n© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization\\nconference. The final version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/\\nListing 3: The prompt of the agent acting as a designer\\nYou are a data video designer. You have a static visualization and corresponding insightful narration text at hand, as\\nwell as the original data table. If necessary , embellish the visualization with corresponding annotations (e.g.,\\narrow, text, circle, etc.) to tell the story more vividly in the narration text. Please think step by step.\\nThe visualization is: {{visualization}}\\nThe insightful narration is: {{narration}}\\nThe data table is: {{table}}\\nTask 1: You want some animation to appear on the visual elements during the audio narration of the video to aid in\\ntelling the data story.\\nConsider the narration as a timeline for the video. Insert animations inside the narration\\ntext where you feel they are needed. The corresponding animation will be triggered when the video reaches the\\ncorresponding narration segment and ends at the end of the narration segment.\\nNotes for animation generation:\\n- The output JSON format is:\\n[\\n{\\n\"animation\": animation type,\\n\"narration\": narration segment ,\\n\"target\": the visual elements that the animation applies to,\\n\"index\": a list of related data table rows index. If empty, output [],\\n\"explanation\": the explanation of why the animation needs to be added\\n}\\n]\\n- One sentence in the narration can correspond to multiple animations.\\n- Output only the part marked with animation.\\n- Narration text cannot be modified.\\n- There are three types of animations: entrance , emphasis , and exit.\\n- Entrance animations include [Axes-fade-in, Bar-grow-in, Line-wipe-in, Pie-wheel-in, Pie-wheel-in-and-legend -fly-in,\\nScatter -fade-in, Bar-grow-and-legend-fade-in, Line-wipe-and-legend -fade-in, Fade-in, Float-in, Fly-in, Zoom-in].\\n- Emphasis animations include [Bar-bounce, Zoom-in-then-zoom-out, Shine-in-a-short-duration , Highlight -one-and-fade-\\nothers].\\n- Exit animations include [Fade-out].\\n- [Axes fade in] animations can only be used at the beginning (first sentence) of a whole narration.\\n- Visual elements that have an entrance animation effect applied will not appear on the canvas until the animation is\\ntriggered. Elements that have an exit animation effect applied will disappear from the canvas after the animation.\\nElements that do not have any animation effect applied will appear on the canvas by default.\\n- Visual elements can only be emphasized or disappear after they appear on the canvas , and elements cannot be emphasized\\nafter they disappear.\\nTask 2: Visualization embellishment generation\\nNotes for generation:\\n- Add annotation only when you think you need to, or export the original visualization if you do not feel the need to\\nadd it.\\n- Tag the narration text in the format of:\\n[\\n{\\n\"type\": a list of annotation types, which is one or a set from [mark label, circle , text, rule, trend line, arrow],\\n\"description\": annotation description and explanation ,\\n\"index\": a list of related data table rows index, If empty, output [],\\n\"nar\": narration segment\\n}\\n]\\n- The tagged annotations must correspond to the annotations in the visualization.\\n- Only output the narration segments marked with annotation. If the value of key \"type\" is [], do not output the item.\\n- If the annotated vega-lite specification has a key \"layer\", then all \"mark\" and \"encoding\" keys should be inside the\\nlist value of the key \"layer\".\\n- The annotations should not be complex.\\n- The annotation must correspond to the narration segment. The annotation will appear when the video reaches the\\ncorresponding narration segment.\\n- The text annotation should be short (e.g., fewer than 6 words).\\n- Please output the complete bug-free vega-lite specification.\\nThe final output JSON format is:\\n{\\n\"Annotated_Visualization\": [A],\\n\"Annotated_Narration_for_Animation\": [B],\\n\"Annotated_Narration_for_Annotation\": [C]\\n}\\nwhere [A] is the generated annotated Vega-Lite specification , [B] is the tagged narration text for animation , and [C] is\\nthe tagged narration text for annotation.\\n8\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_text = get_pdf_text([\"video_stories.pdf\"])\n",
    "pdf_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_chunks(raw_text):\n",
    "    character_text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = character_text_splitter.split_text(raw_text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE\\nVisualization conference. The final version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/\\nFrom Data to Story: Towards Automatic Animated Data Video Creation\\nwith LLM-based Multi-Agent Systems\\nLeixian Shen\\n*\\nThe Hong Kong University\\nof Science and Technology,\\nHong Kong SAR, China\\nHaotian Li\\n†\\nThe Hong Kong University\\nof Science and Technology,\\nHong Kong SAR, China\\nYun Wang\\n‡\\nMicrosoft,\\nBeijing, China\\nHuamin Qu\\n§\\nThe Hong Kong University\\nof Science and Technology,\\nHong Kong SAR, China\\nABSTRACT\\nCreating data stories from raw data is challenging due to humans’\\nlimited attention spans and the need for specialized skills. Recent\\nadvancements in large language models (LLMs) offer great oppor-\\ntunities to develop systems with autonomous agents to streamline\\nthe data storytelling workflow. Though multi-agent systems have',\n",
       " 'advancements in large language models (LLMs) offer great oppor-\\ntunities to develop systems with autonomous agents to streamline\\nthe data storytelling workflow. Though multi-agent systems have\\nbenefits such as fully realizing LLM potentials with decomposed\\ntasks for individual agents, designing such systems also faces chal-\\nlenges in task decomposition, performance optimization for sub-\\ntasks, and workflow design.\\nTo better understand these issues,\\nwe develop Data Director, an LLM-based multi-agent system de-\\nsigned to automate the creation of animated data videos, a repre-\\nsentative genre of data stories. Data Director interprets raw data,\\nbreaks down tasks, designs agent roles to make informed deci-\\nsions automatically, and seamlessly integrates diverse components\\nof data videos. A case study demonstrates Data Director’s effec-\\ntiveness in generating data videos. Throughout development, we\\nhave derived lessons learned from addressing challenges, guiding',\n",
       " 'of data videos. A case study demonstrates Data Director’s effec-\\ntiveness in generating data videos. Throughout development, we\\nhave derived lessons learned from addressing challenges, guiding\\nfurther advancements in autonomous agents for data storytelling.\\nWe also shed light on future directions for global optimization,\\nhuman-in-the-loop design, and the application of advanced multi-\\nmodal LLMs.\\nIndex Terms: Data Storytelling, LLM, Multi-Agent, Data Video\\n1\\nINTRODUCTION\\nThe rapid growth of data assets has driven advancements in various\\ndomains, but it has also presented challenges for human-data inter-\\naction. Humans have limited attention spans and may lack the spe-\\ncialized skills to extract valuable insights and craft engaging data\\nstories across multiple modalities [11]. Automating the generation\\nof stories from raw data can greatly enhance the efficiency of data\\nanalysis and information communication.\\nRecently, advancements in large language models (LLMs) have',\n",
       " 'of stories from raw data can greatly enhance the efficiency of data\\nanalysis and information communication.\\nRecently, advancements in large language models (LLMs) have\\nshowcased robust natural language understanding and reasoning ca-\\npabilities, proving effective across various tasks like data analy-\\nsis [34, 4], document generation [14], and visualization creation [7].\\nThese capabilities open up new avenues to streamline the entire\\ndata storytelling workflow by developing systems featuring LLM-\\npowered autonomous agents. In this paradigm, LLMs serve as the\\ncognitive core of these agents, enabling them to perceive environ-\\nments (Perception), make decisions (Brain), and take responsive\\nactions (Action), thereby assisting humans in automating a wide\\nrange of tasks [35].\\nTherefore, we aim to explore the potential of LLM-based au-\\ntonomous agents in facilitating end-to-end storytelling directly\\n*e-mail: lshenaj@connect.ust.hk\\n†e-mail: haotian.li@connect.ust.hk',\n",
       " 'Therefore, we aim to explore the potential of LLM-based au-\\ntonomous agents in facilitating end-to-end storytelling directly\\n*e-mail: lshenaj@connect.ust.hk\\n†e-mail: haotian.li@connect.ust.hk\\n‡e-mail: wangyun@microsoft.com\\n§e-mail: huamin@cse.ust.hk\\nFigure 1: Architecture of Data Director.\\nfrom raw data, which is a new problem in the visualization and sto-\\nrytelling community. In this paper, we specifically focus on a rep-\\nresentative genre of data stories [19], animated data videos, which\\nencompass diverse components and necessitate the coordination of\\nthese diverse elements [6]. Existing automatic methods for creating\\ndata videos either require users to prepare various materials from\\nraw data [25, 32, 26] or still involve complex and time-consuming\\nmanual authoring processes [5, 30, 8, 27, 3]. We envision that au-\\ntonomous agents can facilitate the automatic transformation of raw\\ndata into animated data videos. However, achieving this goal in-\\nvolves overcoming several challenges:',\n",
       " 'tonomous agents can facilitate the automatic transformation of raw\\ndata into animated data videos. However, achieving this goal in-\\nvolves overcoming several challenges:\\n• Task Decomposition: Data storytelling involves the generation\\nand coordination of diverse elements such as visualizations, text\\nnarrations, audio, and animations. The system should accurately\\ninterpret raw data, break down the storytelling task into manage-\\nable sub-tasks, and assign appropriate roles to agents specialized\\nin handling specific aspects of the task.\\n• Performance Optimization: In each sub-task, the agent is re-\\nquired to make informed decisions based on perception inputs\\nand determine the appropriate tools and methods to use. Each\\nsub-task often relies on the outputs of preceding stages, high-\\nlighting the interdependence among these sub-tasks. So ensuring\\noptimal performance for each one is crucial.\\n• Workflow Design:\\nStorytelling involves numerous intercon-',\n",
       " 'lighting the interdependence among these sub-tasks. So ensuring\\noptimal performance for each one is crucial.\\n• Workflow Design:\\nStorytelling involves numerous intercon-\\nnected sub-tasks with diverse sequence schemes. Tasks such as\\ndata visualization, crafting narration, recording audio, design-\\ning animations, and aligning diverse components are typically\\nnon-linear and their order may vary, presenting challenges in de-\\ntermining the optimal approach. The system needs to facilitate a\\nseamless workflow that automates and effectively integrates all\\nthese sub-tasks.\\nTo better understand these issues, we develop Data Director, an\\n1\\narXiv:2408.03876v1  [cs.HC]  7 Aug 2024\\n© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization\\nconference. The final version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/\\nLLM-based multi-agent system that automates the entire process of',\n",
       " 'conference. The final version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/\\nLLM-based multi-agent system that automates the entire process of\\ntransforming raw data into engaging animated data videos. The sys-\\ntem’s architecture is shown in Fig. 1. Specifically, we decompose\\ndata video creation into distinct sub-tasks based on data video com-\\nponents and their relationships. We design two agent roles—data\\nanalyst and designer—to manage and conduct these sub-tasks, and\\noptimize the performance of each sub-task through prompt design.\\nWe also explore effective ways to interconnect these sub-tasks and\\niteratively refine the workflow. To demonstrate the effectiveness of\\nData Director, we conduct a case study where Data Director gen-\\nerates a data video about real-world stock price data. Finally, we\\nsummarize the lessons learned from our system design for task de-\\ncomposition, performance optimization, and workflow design, and',\n",
       " 'erates a data video about real-world stock price data. Finally, we\\nsummarize the lessons learned from our system design for task de-\\ncomposition, performance optimization, and workflow design, and\\nprovide insights to guide future developments in multi-agent sys-\\ntems for the automatic transformation of raw data into data stories.\\n2\\nDATA DIRECTOR\\nThis section will first give an overview of Data Director’s architec-\\nture and then introduce our design practices.\\n2.1\\nOverview\\nData Director is an LLM-based multi-agent system powered by\\nGPT-4 [2].\\nWe follow existing conceptual framework of LLM-\\nbased agent to design three components: perception, brain, and ac-\\ntion [35]. The architecture of Data Director is illustrated in Fig. 1,\\nwith a central controller (a) scheduling all components.\\nUser-\\ngenerated data (b) is directly input into Data Director. The percep-\\ntion module (c) preprocesses the data, which is then fed into the first',\n",
       " 'User-\\ngenerated data (b) is directly input into Data Director. The percep-\\ntion module (c) preprocesses the data, which is then fed into the first\\nagent acting as a data analyst (d). This agent’s tasks include extract-\\ning insights, visualizing data, and crafting narration text. The gen-\\nerated visualization and narration text are passed to the next agent,\\nwhich acts as a designer (e). This agent is responsible for animating\\nand annotating the content, as well as coordinating the data video\\ncomponents. Finally, the controller (a) utilizes the decisions made\\nby the multi-agent system to generate a data video with relevant\\ntools (f).\\n2.2\\nTask Decomposition and Workflow Design\\nTo design multi-agent systems for complex tasks, such as data sto-\\nrytelling, it is crucial to decompose the process into patterned sub-\\ntasks, facilitating model learning and interpretation [35]. Task de-\\ncomposition is a balance art between accuracy and efficiency.',\n",
       " 'rytelling, it is crucial to decompose the process into patterned sub-\\ntasks, facilitating model learning and interpretation [35]. Task de-\\ncomposition is a balance art between accuracy and efficiency.\\nCoarse tasks may exceed the model’s capabilities, leading to hal-\\nlucinations or non-computational results, while excessively fine-\\ngrained tasks may overwhelm the model with excessive tasks, af-\\nfecting efficiency and increasing costs.\\nTo decompose the tasks in the context of data video creation,\\nwe first break down data videos into basic components and the\\nrelationship between these components, following previous re-\\nsearch [32, 25]. The basic components of data videos include data\\nvisualizations, text narrations, and visual animations. To make sure\\nthese components appear coherently in data videos, three relation-\\nships need to be taken care of: 1) Animated visualization elements\\nmust semantically connect with corresponding text narration seg-',\n",
       " 'these components appear coherently in data videos, three relation-\\nships need to be taken care of: 1) Animated visualization elements\\nmust semantically connect with corresponding text narration seg-\\nments; 2) Animation effects must be tailored to the visualization el-\\nements they accompany; 3) Text narration should align temporally\\nwith the animations, serving as the timeline.\\nWith these identified components and relationships, we assign\\nthe three component creation tasks and the three relationship man-\\nagement tasks (six sub-tasks in total) to two LLM-powered agents,\\nassuming the roles of a data analyst and a designer. Additionally,\\nwe develop a controller to manage the input and output of each\\nmodule, parse outputs, and invoke appropriate tools for tasks be-\\nyond LLM capabilities.\\nFig. 2 illustrates a case study based on real-world stock price\\ndata of four IT companies.\\nPerception. The perception module accepts and processes diverse',\n",
       " 'yond LLM capabilities.\\nFig. 2 illustrates a case study based on real-world stock price\\ndata of four IT companies.\\nPerception. The perception module accepts and processes diverse\\ninformation from external environments, transforming it into un-\\nderstandable representations for LLMs [35]. In Data Director, data\\ntables are inputted directly in the prompt as formatted text. During\\nthe perception phase (A), a data preprocessing module utilizes the\\ndataset’s title and content within an LLM session to generate nat-\\nural language descriptions. As shown in Fig. 2-A, the description\\ninvolves a high-level overview of the data topic, along with a de-\\ntailed elaboration of the semantics of each data column, enhancing\\ncontextual information for subsequent model processes. The gener-\\nated NL data description and the raw data are then fed into the data\\nanalysis brain.\\nRole as Data Analyst.\\nInspired by insight-based visualization\\nand understanding techniques [39, 33], when analyzing data, Data',\n",
       " 'analysis brain.\\nRole as Data Analyst.\\nInspired by insight-based visualization\\nand understanding techniques [39, 33], when analyzing data, Data\\nDirector first prompts the LLM to extract top-k interesting data\\ninsights (B) from raw data, based on data analysis task model-\\ning [23, 21]. These insights guide the generation of visualizations\\n(sub-task 1) and text narration (sub-task 2). Following the Chain-\\nof-Thought strategy [9] and carefully designed LLM prompts, the\\nbrain incrementally derives a list of insights (B), declarative Vega-\\nLite visualizations [18] (C), and text narrations (D) step by step.\\nRole as Designer. Once the “data analyst” has prepared visual-\\nizations and corresponding text narrations, the “designer” agent fo-\\ncuses on creating dynamic animations (sub-task 3) and synchroniz-\\ning components (sub-task 4 5 6). Animations are categorized into\\ntwo types: visual effects (e.g., fade, grow, fly, zoom, etc.) applied',\n",
       " 'cuses on creating dynamic animations (sub-task 3) and synchroniz-\\ning components (sub-task 4 5 6). Animations are categorized into\\ntwo types: visual effects (e.g., fade, grow, fly, zoom, etc.) applied\\nto visualization elements, and annotations that introduce additional\\nvisual elements at the right moments. Animation effects are de-\\ntermined by distilling choices from an animation library into nat-\\nural language prompts (E). The agent is tasked with selecting the\\noptimal timing for animation applications, identifying the precise\\nvisual elements that will be animated, and choosing the appropri-\\nate animation effects. Furthermore, given the complexity of Vega-\\nLite-specified annotated visualizations, we adopt a hierarchical ap-\\nproach, initially generating base visualizations with the data ana-\\nlyst agent and subsequently enhancing them with annotations (with\\nVega-Lite specifications) for improved outcomes (F) using the de-',\n",
       " 'proach, initially generating base visualizations with the data ana-\\nlyst agent and subsequently enhancing them with annotations (with\\nVega-Lite specifications) for improved outcomes (F) using the de-\\nsigner agent. Text narration serves as a timeline, transforming tem-\\nporal synchronization into semantic links between static narration\\nsegments and visual elements [32, 25].\\nController. As shown in the middle of Fig. 2, based on the NL\\noutputs of the models, the controller first utilizes text-to-speech ser-\\nvices to convert text narrations into audio with precise timestamps,\\nensuring alignment with the data video timeline. Then, the con-\\ntroller invokes a visualization renderer to convert Vega-Lite spec-\\nifications into SVG files. Each SVG element is automatically as-\\nsociated with blackened data and visualization structure informa-\\ntion [25, 28]. Narration segments are further linked to these SVG\\nvisual elements based on the text-visual links established by the',\n",
       " 'sociated with blackened data and visualization structure informa-\\ntion [25, 28]. Narration segments are further linked to these SVG\\nvisual elements based on the text-visual links established by the\\ndesigner agent, akin to Data Player [25]. Then the corresponding\\nanimation effects are applied to the SVG elements based on the de-\\nsigner agent’s decisions. Next, annotated visualizations are parsed\\nto detect SVG annotation elements, integrating “fade in” anima-\\ntions at corresponding timestamps. Finally, the controller calls a\\nvideo synthesizer to merge visualizations, audio narration, and ani-\\nmation sequences into the final data video.\\n3\\nCASE STUDY\\nFig. 2 illustrates a real-world case study using stock price data. Data\\nDirector generates diverse insights (B) and visualizes stock prices\\nover time using line charts (C). The narration followed a structured\\napproach (C): starting with an overview, detailing each company’s\\nperformance, and concluding with a summary. Based on the nar-',\n",
       " 'over time using line charts (C). The narration followed a structured\\napproach (C): starting with an overview, detailing each company’s\\nperformance, and concluding with a summary. Based on the nar-\\nration, points and text annotations were added to each company’s\\nline to highlight its notable characteristic (F). The bottom of Fig. 2\\ndisplays key animation frames (E) from the video, with each frame\\n2\\n© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization\\nconference. The final version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/\\nFigure 2: An example walkthrough of Data Director.\\nnumbered to correspond with specific timestamps in the narration\\n(D). This demonstrates when specific animations are triggered. The\\nfinal data video starts with an entrance animation showcasing all\\nelements, and when discussing each company, the respective line\\nis highlighted individually. Overall, the entire narrative is well-',\n",
       " 'final data video starts with an entrance animation showcasing all\\nelements, and when discussing each company, the respective line\\nis highlighted individually. Overall, the entire narrative is well-\\ncrafted, with smooth articulation and appropriately designed visu-\\nalizations and animations, resulting in an engaging data video.\\n4\\nLESSONS LEARNED\\nThis section will discuss the lessons learned throughout the devel-\\nopment of Data Director, focusing on task decomposition, perfor-\\nmance optimization, and workflow design.\\n4.1\\nTask Decomposition\\nBalancing Accuracy and Efficiency. Task decomposition for data\\nstorytelling requires balancing accuracy and efficiency. When de-\\ncomposing tasks, first, it is essential to identify which tasks the\\nmodel excels in (e.g., natural language generation, reasoning, and\\ntext-based decision-making) and distinguish these from tasks that\\nnecessitate external tools (e.g., visualization rendering, audio gen-\\neration, and video synthesizing).',\n",
       " 'text-based decision-making) and distinguish these from tasks that\\nnecessitate external tools (e.g., visualization rendering, audio gen-\\neration, and video synthesizing).\\nSecond, the sub-tasks result-\\ning from decomposition should be well-defined and manageable.\\nThese sub-tasks can then be grouped to shape agent roles that\\nalign with the inherent characteristics of the tasks. For example,\\nData Director breaks down tasks based on the data video compo-\\n3\\n© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization\\nconference. The final version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/\\nnents, and organizes sub-tasks into analysis-focused tasks that gen-\\nerate static content from raw data, and design-focused tasks that\\nrequire creative input and the derivation of dynamic effects from\\nthe static content. Third, the suitable combination of sub-tasks can',\n",
       " 'erate static content from raw data, and design-focused tasks that\\nrequire creative input and the derivation of dynamic effects from\\nthe static content. Third, the suitable combination of sub-tasks can\\nenhance both the model’s accuracy and efficiency, as demonstrated\\nin Sec. 2.2, where animation design and temporal synchronization\\nwere merged. Finally, a top-down approach can be adopted to dis-\\nsect tasks progressively, designing suitable and efficient models for\\neach sub-task.\\nData Feeding with Contextual Information. Providing the model\\nwith ample contextual information has been found to enhance the\\naccuracy and quality of its generation [34]. As the model progresses\\nthrough sub-tasks step by step, the context is enriched and updated,\\noffering more information for subsequent operations. For example,\\nin the data analyst agent, the model gradually gathers information\\non data descriptions, insights, visualizations, and narrations. How-',\n",
       " 'in the data analyst agent, the model gradually gathers information\\non data descriptions, insights, visualizations, and narrations. How-\\never, when an agent perceives data from the environment, the data\\nitself is merely numerical with limited context. We have found that\\nsemantically enriching the data and supplying the model with con-\\ntextual insights significantly enhance its effectiveness. For exam-\\nple, Data Director uses the LLM to generate an NL description of\\na data table with a title. Future research could integrate innovative\\ntechniques for enhancing data comprehension and exploring novel\\nmethods for inputting data into LLMs.\\n4.2\\nPerformance Optimization\\nEffective prompt design is crucial for enhancing the performance\\nand output quality of LLM-based agent systems. The complete\\nprompt of Data Director can be found in Appendix A. The key\\nstrategies for optimizing prompts within this context are outlined\\nas follows:\\nAssignment of Appropriate Tasks for LLMs. The foundation',\n",
       " 'prompt of Data Director can be found in Appendix A. The key\\nstrategies for optimizing prompts within this context are outlined\\nas follows:\\nAssignment of Appropriate Tasks for LLMs. The foundation\\nof effective prompt design lies in the careful design of tasks for\\nLLMs. It is essential to identify the tasks where LLMs are good\\nat. Furthermore, assigning the model a specific role, such as a data\\nanalyst or designer in Data Director, can guide the model to pro-\\nduce domain-specific and contextually relevant outputs. In addi-\\ntion, supplying the model with precise and comprehensive context\\ncan enhance its understanding of tasks, thereby improving the ac-\\ncuracy and relevance of its responses. This involves crafting well-\\nstructured prompts (outlined in Appendix A) and designing com-\\nplementary modules like data preprocessing in Data Director.\\nCognitive Processing Time and Task Decomposition.\\nAllow-\\ning the model adequate cognitive processing time is essential for',\n",
       " 'plementary modules like data preprocessing in Data Director.\\nCognitive Processing Time and Task Decomposition.\\nAllow-\\ning the model adequate cognitive processing time is essential for\\nachieving high-quality outputs and alleviating hallucinations. Be-\\nyond the task decomposition discussed above, in terms of prompt\\ndesign, the sub-tasks can be clearly defined with sequential steps,\\nfacilitating the application of the Chain-of-Thought strategy. More-\\nover, the number of tasks within one prompt should be balanced to\\ndeduce task difficulty. In addition, prompting the model to explain\\nits decisions or outline its solution methodology before concluding\\ncan promote a more thoughtful and precise response generation pro-\\ncess. For instance, in the designer agent, Data Director prompts the\\nLLM to its choices regarding the animation and annotation design,\\nwhich enhances the decision accuracy and simplifies the debugging\\nof LLM applications during development.',\n",
       " 'LLM to its choices regarding the animation and annotation design,\\nwhich enhances the decision accuracy and simplifies the debugging\\nof LLM applications during development.\\nCrafting Precise and Unambiguous Instructions. The clarity and\\nspecificity of instructions are paramount in effective prompt design.\\nUtilizing delimiters (e.g., “‘ “‘, “ ” or <>) to segment prompt sec-\\ntions can reduce ambiguity and aid comprehension. Providing fine-\\ngrained requirements and employing the correct use of keywords\\n(e.g., “summarize” vs. “extract”) ensures that the model adheres\\nclosely to the task parameters. Furthermore, requesting structured\\noutputs (e.g., JSON and HTML) and offering a range of response\\noptions can guide the model toward producing organized and prac-\\ntical outputs. For example, as shown in Fig. 2, Data Director allows\\nLLMs to select insight types (B), animation types (E), and anno-\\ntation types (F) from a set of predefined candidates. Additionally,',\n",
       " 'LLMs to select insight types (B), animation types (E), and anno-\\ntation types (F) from a set of predefined candidates. Additionally,\\nincorporating conditional logic (e.g., if-else statements), employing\\nfew-shot or one-shot prompting techniques with curated examples,\\nand referencing URLs for concrete examples can further enhance\\nthe model’s understanding and task execution accuracy. More spe-\\ncific examples can be found in Appendix A.\\n4.3\\nWorkflow Design\\nShared Representation. This paper primarily utilizes the GPT-4\\nmodel [2], with natural language serving as the medium for input\\nand output, setting the stage for this discussion. Effective commu-\\nnication between agents and between agents and external tools re-\\nquires an appropriate shared NL representation. For instance, Vega-\\nLite is employed in Data Director to represent all visualizations and\\nannotations, while insights and animated visual information are en-\\ncapsulated in a JSON format, incorporating specific feature infor-',\n",
       " 'annotations, while insights and animated visual information are en-\\ncapsulated in a JSON format, incorporating specific feature infor-\\nmation (see Fig. 2). Such shared representations must be compre-\\nhensible and easily generated by the model and readily interpreted\\nby external tools for mapping to internal operations. Future work\\ncould involve designing a global shared representation for specific\\napplication scenarios to assist models in better preserving and gen-\\nerating contextual information.\\nIterative Development.\\nVarious sub-tasks within the workflow\\npresent diverse sequencing strategies. For instance, annotations can\\nbe generated simultaneously with visualizations, during animation\\ngeneration, or using a hierarchical approach as described in Data\\nDirector. Similarly, in data analysis, one may opt to produce vi-\\nsualizations either before or after narration. Designing the optimal\\nsequencing strategy is challenging due to the absence of a quantita-',\n",
       " 'sualizations either before or after narration. Designing the optimal\\nsequencing strategy is challenging due to the absence of a quantita-\\ntive global optimization objective. Hence, for applications of LLM-\\nbased agents, we adopt an iterative design methodology based on\\ntask decomposition and local performance optimization [1]. This\\ninvolves a cycle of ideation, implementation, experimental evalu-\\nation, and error analysis. Striving for consistency in the model’s\\noutputs also necessitates adherence to established guidelines and\\nmeticulous parameter adjustments. We note that Data Director pre-\\nsented here is the result of our iterative optimizations and may not\\nnecessarily represent the optimal configuration. Our intention in\\ndeveloping this prototype tool is to uncover valuable lessons and\\ninsights that can guide future research.\\n5\\nFUTURE WORK\\nGlobal Optimization and Benchmarking. The iterative develop-\\nment of multi-agent systems, as mentioned in Sec. 2.2, suffers from',\n",
       " 'insights that can guide future research.\\n5\\nFUTURE WORK\\nGlobal Optimization and Benchmarking. The iterative develop-\\nment of multi-agent systems, as mentioned in Sec. 2.2, suffers from\\nthe lack of a global optimization and validation framework for end-\\nto-end data video generation [16]. Additionally, the community\\nlacks a widely recognized benchmark. The complexity of this chal-\\nlenge is compounded by the inherently subjective nature of data\\nstorytelling quality, which is subject to individual interpretation\\nand the multifaceted decision-making involved in various narrative\\nforms. Future work could include summarizing relevant rubrics and\\nconducting empirical studies to derive quantitative guidelines. With\\nthese well-defined metrics, an evaluation agent can also be added to\\nenhance existing workflow. Additionally, there is a need to develop\\na universally shared representation for optimization and incorpo-\\nrate domain-specific languages and objectives tailored to diverse',\n",
       " 'enhance existing workflow. Additionally, there is a need to develop\\na universally shared representation for optimization and incorpo-\\nrate domain-specific languages and objectives tailored to diverse\\nscenarios [15, 17].\\nHuman-in-the-Loop. Data-driven end-to-end generation solutions\\ncan result in one-size-fits-all outputs. To address the issues, in-\\ncorporating human-in-the-loop is an essential approach to com-\\npensate for model limitations and generate more personalized re-\\nsults [11, 10]. In data storytelling, three paradigms of human-in-\\nthe-loop can be further explored: firstly, allowing users to input\\nmore information in the perception module while maintaining the\\ncurrent architecture, articulating their goals and requirements in the\\n4\\n© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization\\nconference. The final version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/',\n",
       " 'conference. The final version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/\\nforms like natural language [20], example [36, 22], and sketch [14];\\nsecondly, integrating humans into sub-tasks to achieve local opti-\\nmization before proceeding to the next stage, such as generating\\nmultiple candidates for visualization and annotation after generat-\\ning data insights; thirdly, users providing conversational feedback\\nbased on the output [20, 31], with the agent generating new end-to-\\nend results based on this feedback. Additionally, these methods can\\nalso be flexibly combined.\\nKeeping Up with Cutting-Edge Models. This paper primarily\\nuses the GPT-4 model. However, with the rapid evolution of large\\nlanguage models (LLMs), GPT-4 is swiftly being augmented by\\nthe emergence of multimodal LLMs [38]. These advanced models\\noffer expanded functionalities for handling multimodal inputs and\\noutputs, significantly impacting task decomposition, performance',\n",
       " 'the emergence of multimodal LLMs [38]. These advanced models\\noffer expanded functionalities for handling multimodal inputs and\\noutputs, significantly impacting task decomposition, performance\\noptimization, and workflow design within our established frame-\\nwork (Fig. 1). For instance, initial generation of visualization files\\ncould be followed by refinement in a subsequent multimodal mod-\\nule, potentially leading to direct generation of video content. The\\nenhancement of model capabilities presents numerous opportuni-\\nties. Future work should not only track the latest models to develop\\nmore powerful agents but also leverage diverse models with differ-\\nent capabilities to enrich data storytelling. This includes expanding\\nbeyond individual static charts to incorporate visual and musical\\ncontent [29], supporting more complex insights and multi-view vi-\\nsualizations [13], integrating existing computational design spaces\\n(e.g., camera [12] and narrative structure [37]), and accommodating',\n",
       " 'sualizations [13], integrating existing computational design spaces\\n(e.g., camera [12] and narrative structure [37]), and accommodating\\nmore data types (e.g., unstructured graphs [24]). Achieving these\\nfeatures requires enhancing shared representations and designing\\ncorresponding prompts (similar to how Data Director integrates an-\\nimation).\\nInherent Limitations of Large Language Models.\\nLLMs are\\npowerful but exhibit several inherent limitations, such as error accu-\\nmulation, inconsistent results, hallucinations, and high time costs.\\nMost importantly, we need to acknowledge that the content from\\nLLMs is generative but not truthful. To address error accumulation,\\nincorporating a human-in-the-loop approach and providing timely\\ntips can improve accuracy. Consistency in results can be improved\\nby strictly following established guidelines and creating supple-\\nmentary rules to handle the model’s output. Hallucinations, where\\nthe model generates plausible but incorrect information, can be im-',\n",
       " 'by strictly following established guidelines and creating supple-\\nmentary rules to handle the model’s output. Hallucinations, where\\nthe model generates plausible but incorrect information, can be im-\\nproved by implementing some prompt optimization strategies, such\\nas self-repair mechanisms, the Chain-of-Thought (CoT) approach,\\nand code-interpreter functionalities [34]. Lastly, high-time costs\\ncan be managed by breaking down tasks, finding suitable solutions\\nfor each (e.g., heuristics, basic models, and LLMs). It’s important to\\nrecognize that LLMs are not a one-size-fits-all solution; sometimes,\\nbasic models or heuristic rules can be highly effective without the\\nneed for LLMs.\\n6\\nCONCLUSION\\nThe rapid evolution of LLMs presents new opportunities for creat-\\ning end-to-end multi-agent systems for data storytelling. Through\\nthe development of Data Director, we have derived valuable in-\\nsights into task decomposition, local performance optimization\\nthrough prompt design, and workflow design.',\n",
       " 'the development of Data Director, we have derived valuable in-\\nsights into task decomposition, local performance optimization\\nthrough prompt design, and workflow design.\\nIn addition, we\\nalso shed light on future directions in the development of glob-\\nally optimized multi-agents, human-in-the-loop systems, integra-\\ntion of cutting-edge multimodal models, and addressing inherent\\nLLM limitations.\\nACKNOWLEDGMENTS\\nThe authors wish to thank all reviewers for their valuable feed-\\nback. This work has been partially supported by RGC GRF Grant\\n16210321.\\nREFERENCES\\n[1] Chatgpt prompt engineering for developers.\\nhttps://learn.\\ndeeplearning.ai/chatgpt-prompt-eng/. 4\\n[2] Openai gpt-4. https://openai.com/gpt-4. 2, 4\\n[3] F. Amini, N. H. Riche, B. Lee, A. Monroy-Hernandez, and P. Irani.\\nAuthoring data-driven videos with dataclips. IEEE Transactions on\\nVisualization and Computer Graphics, 23(1):501–510, 2017. 1\\n[4] C. Beasley and A. Abouzied. Pipe(line) Dreams: Fully Automated',\n",
       " 'Authoring data-driven videos with dataclips. IEEE Transactions on\\nVisualization and Computer Graphics, 23(1):501–510, 2017. 1\\n[4] C. Beasley and A. Abouzied. Pipe(line) Dreams: Fully Automated\\nEnd-to-End Analysis and Visualization. In Proceedings of the 2024\\nWorkshop on Human-In-the-Loop Data Analytics, pp. 1–7. ACM,\\n2024. 1\\n[5] Y. Cao, J. L. E, Z. Chen, and H. Xia. DataParticles: Block-based\\nand Language-oriented Authoring of Animated Unit Visualizations.\\nIn Proceedings of the 2023 CHI Conference on Human Factors in\\nComputing Systems, CHI ’23, pp. 1–15. ACM, 2023. 1\\n[6] H. Cheng, J. Wang, Y. Wang, B. Lee, H. Zhang, and D. Zhang. In-\\nvestigating the role and interplay of narrations and animations in data\\nvideos. Computer Graphics Forum, 41(3):527–539, 2022. 1\\n[7] V. Dibia.\\nLIDA: A Tool for Automatic Generation of Grammar-\\nAgnostic Visualizations and Infographics using Large Language Mod-\\nels. In Proceedings of the 61st Annual Meeting of the Association for',\n",
       " '[7] V. Dibia.\\nLIDA: A Tool for Automatic Generation of Grammar-\\nAgnostic Visualizations and Infographics using Large Language Mod-\\nels. In Proceedings of the 61st Annual Meeting of the Association for\\nComputational Linguistics, ACL’23, pp. 113–126. ACL, 2023. 1\\n[8] T. Ge, B. Lee, and Y. Wang. CAST: Authoring Data-Driven Chart\\nAnimations. In Proceedings of the 2021 CHI Conference on Human\\nFactors in Computing Systems, CHI ’21, pp. 1–15. ACM, 2021. 1\\n[9] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large\\nlanguage models are zero-shot reasoners. In S. Koyejo, S. Mohamed,\\nA. Agarwal, D. Belgrave, K. Cho, and A. Oh, eds., Proceedings of Ad-\\nvances in Neural Information Processing Systems, NIPS’22, vol. 35,\\npp. 22199–22213, 2022. 2\\n[10] H. Li, Y. Wang, Q. V. Liao, and H. Qu. Why is AI not a Panacea for\\nData Workers? An Interview Study on Human-AI Collaboration in\\nData Storytelling. arXiv preprint arXiv:2304.08366, 2023. 4',\n",
       " '[10] H. Li, Y. Wang, Q. V. Liao, and H. Qu. Why is AI not a Panacea for\\nData Workers? An Interview Study on Human-AI Collaboration in\\nData Storytelling. arXiv preprint arXiv:2304.08366, 2023. 4\\n[11] H. Li, Y. Wang, and H. Qu. Where Are We So Far? Understand-\\ning Data Storytelling Tools from the Perspective of Human-AI Col-\\nlaboration. In Proceedings of CHI Conference on Human Factors in\\nComputing Systems, CHI’24, pp. 1–28, 2024. 1, 4\\n[12] W. Li, Z. Wang, Y. Wang, D. Weng, L. Xie, S. Chen, H. Zhang, and\\nH. Qu. GeoCamera: Telling Stories in Geographic Visualizations with\\nCamera Movements. In Proceedings of CHI Conference on Human\\nFactors in Computing Systems, CHI’23, pp. 1–15. ACM, 2023. 5\\n[13] Y. Lin, H. Li, A. Wu, Y. Wang, and H. Qu. DMiner: Dashboard Design\\nMining and Recommendation. IEEE Transactions on Visualization\\nand Computer Graphics, 30(7):4108–4121, 2024. 5\\n[14] Y. Lin, H. Li, L. Yang, A. Wu, and H. Qu.\\nInkSight: Leverag-',\n",
       " 'Mining and Recommendation. IEEE Transactions on Visualization\\nand Computer Graphics, 30(7):4108–4121, 2024. 5\\n[14] Y. Lin, H. Li, L. Yang, A. Wu, and H. Qu.\\nInkSight: Leverag-\\ning Sketch Interaction for Documenting Chart Findings in Computa-\\ntional Notebooks. IEEE Transactions on Visualization and Computer\\nGraphics, 30(1):944 – 954, 2024. 1, 5\\n[15] Y. Ouyang, L. Shen, Y. Wang, and Q. Li.\\nNotePlayer: Engaging\\nJupyter Notebooks for Dynamic Presentation of Analytical Processes.\\narXiv preprint arXiv:2408.01101, pp. 1–15, 2024. 4\\n[16] J. S. Park, J. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S.\\nBernstein. Generative Agents: Interactive Simulacra of Human Be-\\nhavior. In Proceedings of the Annual ACM Symposium on User Inter-\\nface Software and Technology, UIST’23, pp. 1–22. ACM, 2023. 4\\n[17] S. Sallam, Y. Sakamoto, J. Leboe-McGowan, C. Latulipe, and P. Irani.\\nTowards Design Guidelines for Effective Health-Related Data Videos:',\n",
       " '[17] S. Sallam, Y. Sakamoto, J. Leboe-McGowan, C. Latulipe, and P. Irani.\\nTowards Design Guidelines for Effective Health-Related Data Videos:\\nAn Empirical Investigation of Affect, Personality, and Video Content.\\nIn Proceedings of CHI Conference on Human Factors in Computing\\nSystems, CHI’22, pp. 1–22. ACM, 2022. 4\\n[18] A. Satyanarayan, D. Moritz, K. Wongsuphasawat, and J. Heer. Vega-\\nLite: A Grammar of Interactive Graphics. IEEE Transactions on Vi-\\nsualization and Computer Graphics, 23(1):341–350, 2017. 2\\n[19] E. Segel and J. Heer.\\nNarrative visualization: Telling stories with\\ndata. IEEE Transactions on Visualization and Computer Graphics,\\n16(6):1139–1148, 2010. 1\\n[20] L. Shen, E. Shen, Y. Luo, X. Yang, X. Hu, X. Zhang, Z. Tai, and\\nJ. Wang. Towards Natural Language Interfaces for Data Visualiza-\\ntion: A Survey. IEEE Transactions on Visualization and Computer\\nGraphics, 29(6):3121–3144, 2023. 5\\n5',\n",
       " 'J. Wang. Towards Natural Language Interfaces for Data Visualiza-\\ntion: A Survey. IEEE Transactions on Visualization and Computer\\nGraphics, 29(6):3121–3144, 2023. 5\\n5\\n© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization\\nconference. The final version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/\\n[21] L. Shen, E. Shen, Z. Tai, Y. Song, and J. Wang. TaskVis: Task-oriented\\nVisualization Recommendation.\\nIn Proceedings of the 23th Euro-\\ngraphics Conference on Visualization (Short Papers), EuroVis’21, pp.\\n91–95. Eurographics, 2021. 2\\n[22] L. Shen, E. Shen, Z. Tai, Y. Wang, Y. Luo, and J. Wang. GALVIS: Vi-\\nsualization Construction through Example-Powered Declarative Pro-\\ngramming.\\nIn Proceedings of the 31st ACM International Confer-\\nence on Information & Knowledge Management, CIKM’22, pp. 4975–\\n4979. ACM, 2022. 5\\n[23] L. Shen, E. Shen, Z. Tai, Y. Xu, J. Dong, and J. Wang. Visual Data',\n",
       " 'ence on Information & Knowledge Management, CIKM’22, pp. 4975–\\n4979. ACM, 2022. 5\\n[23] L. Shen, E. Shen, Z. Tai, Y. Xu, J. Dong, and J. Wang. Visual Data\\nAnalysis with Task-Based Recommendations. Data Science and En-\\ngineering, 7(4):354–369, 2022. 2\\n[24] L. Shen, Z. Tai, E. Shen, and J. Wang.\\nGraph Exploration With\\nEmbedding-Guided Layouts. IEEE Transactions on Visualization and\\nComputer Graphics, 30(7):3693–3708, 2024. 5\\n[25] L. Shen, Y. Zhang, H. Zhang, and Y. Wang. Data Player: Automatic\\nGeneration of Data Videos with Narration-Animation Interplay. IEEE\\nTransactions on Visualization and Computer Graphics, 30(1):109–\\n119, 2024. 1, 2\\n[26] D. Shi, F. Sun, X. Xu, X. Lan, D. Gotz, and N. Cao. AutoClips: An\\nAutomatic Approach to Video Generation from Data Facts. Computer\\nGraphics Forum, 40(3):495–505, 2021. 1\\n[27] M. Shin, J. Kim, Y. Han, L. Xie, M. Whitelaw, B. C. Kwon, S. Ko,\\nand N. Elmqvist. Roslingifier: Semi-Automated Storytelling for Ani-',\n",
       " 'Graphics Forum, 40(3):495–505, 2021. 1\\n[27] M. Shin, J. Kim, Y. Han, L. Xie, M. Whitelaw, B. C. Kwon, S. Ko,\\nand N. Elmqvist. Roslingifier: Semi-Automated Storytelling for Ani-\\nmated Scatterplots. IEEE Transactions on Visualization and Computer\\nGraphics, 29(6):2980–2995, 2023. 1\\n[28] L. S. Snyder and J. Heer. DIVI: Dynamically Interactive Visualiza-\\ntion. IEEE Transactions on Visualization and Computer Graphics,\\n30(1):403–413, 2024. 2\\n[29] T. Tang, J. Tang, J. Lai, L. Ying, Y. Wu, L. Yu, and P. Ren. SmartShots:\\nAn Optimization Approach for Generating Videos with Data Visual-\\nizations Embedded. ACM Transactions on Interactive Intelligent Sys-\\ntems, 12(1):1–21, 2022. 5\\n[30] J. R. Thompson, Z. Liu, and J. Stasko. Data Animator: Authoring\\nExpressive Animated Data Graphics. In Proceedings of the 2021 CHI\\nConference on Human Factors in Computing Systems, CHI’21, pp.\\n1–18. ACM, 2021. 1\\n[31] Y. Wang, Z. Hou, L. Shen, T. Wu, J. Wang, H. Huang, H. Zhang,',\n",
       " 'Conference on Human Factors in Computing Systems, CHI’21, pp.\\n1–18. ACM, 2021. 1\\n[31] Y. Wang, Z. Hou, L. Shen, T. Wu, J. Wang, H. Huang, H. Zhang,\\nand D. Zhang. Towards Natural Language-Based Visualization Au-\\nthoring. IEEE Transactions on Visualization and Computer Graphics,\\n29(1):1222 – 1232, 2023. 5\\n[32] Y. Wang, L. Shen, Z. You, X. Shu, B. Lee, J. Thompson, H. Zhang, and\\nD. Zhang. WonderFlow: Narration-Centric Design of Animated Data\\nVideos. IEEE Transactions on Visualization and Computer Graphics,\\npp. 1–15, 2024. 1, 2\\n[33] Y. Wang, Z. Sun, H. Zhang, W. Cui, K. Xu, X. Ma, and D. Zhang.\\nDataShot:\\nAutomatic Generation of Fact Sheets from Tabular\\nData. IEEE Transactions on Visualization and Computer Graphics,\\n26(1):895–905, 2020. 2\\n[34] Y. Wu, Y. Wan, H. Zhang, Y. Sui, W. Wei, W. Zhao, G. Xu, and H. Jin.\\nAutomated Data Visualization from Natural Language via Large Lan-\\nguage Models: An Exploratory Study. In Proceedings of the ACM on',\n",
       " 'Automated Data Visualization from Natural Language via Large Lan-\\nguage Models: An Exploratory Study. In Proceedings of the ACM on\\nManagement of Data, SIGMOD’24, pp. 1–28. ACM, 2024. 1, 4, 5\\n[35] Z. Xi, W. Chen, X. Guo, W. He, and et al. The Rise and Potential of\\nLarge Language Model Based Agents: A Survey. arXiv: 2309.07864,\\npp. 1–86, 2023. 1, 2\\n[36] L. Xie, Z. Zhou, K. Yu, Y. Wang, H. Qu, and S. Chen. Wakey-Wakey:\\nAnimate Text by Mimicking Characters in a GIF.\\nIn Proceedings\\nof the 36th Annual ACM Symposium on User Interface Software and\\nTechnology, UIST’23, pp. 1–14. ACM, 2023. 5\\n[37] L. Yang, X. Xu, X. Y. Lan, Z. Liu, S. Guo, Y. Shi, H. Qu, and N. Cao.\\nA Design Space for Applying the Freytag’s Pyramid Structure to Data\\nStories. IEEE Transactions on Visualization and Computer Graphics,\\n28(1):922–932, 2022. 5\\n[38] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen. A survey\\non multimodal large language models. arXiv:2306.13549, 2023. 5',\n",
       " '28(1):922–932, 2022. 5\\n[38] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen. A survey\\non multimodal large language models. arXiv:2306.13549, 2023. 5\\n[39] L. Ying, Y. Wang, H. Li, S. Dou, H. Zhang, X. Jiang, H. Qu, and\\nY. Wu. Reviving Static Charts into Live Charts. IEEE Transactions\\non Visualization and Computer Graphics, pp. 1–15, 2024. 2\\n6\\n© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization\\nconference. The final version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/\\nA\\nFULL PROMPT\\nListing 1: The prompt of generating text description for data tables\\nGive a short and consistent description of the following data table and columns:\\n{{table}}\\nThe title of the data table is: {{title}}\\nThe output JSON format is:\\n{\\n\"Description\": [A]\\n}\\nwhere [A] is the generated description.\\nListing 2: The prompt of the agent acting as a data analyst\\nYou are a data analyst. You have a data table at hand.',\n",
       " '{\\n\"Description\": [A]\\n}\\nwhere [A] is the generated description.\\nListing 2: The prompt of the agent acting as a data analyst\\nYou are a data analyst. You have a data table at hand.\\n{{description}}\\nThe full data table is:\\n{{table}}\\nYou need to complete several tasks, please think step by step.\\nTask 1: Please list the top insights you can gather from the following data table.\\nNotes for insight extraction:\\n- The output JSON format is:\\n[\\n{\\n\"insight\": insight content ,\\n\"type\": a list of corresponding insight types\\n}\\n]\\n- The insight type belongs to the following list: [Change Over Time, Characterize Distribution , Cluster , Comparison ,\\nCorrelate , Determine Range, Deviation , Find Anomalies , Find Extremum , Magnitude , Part to Whole, Sort, Trend]. One\\ninsight can correspond to multiple types.\\n- The selected insights should be obvious , valuable , and diverse.\\n- Double-check that the comparison of numerical magnitudes is accurate. Ensure that the insight is right.\\n- Ignore the \"index\" column.',\n",
       " '- Double-check that the comparison of numerical magnitudes is accurate. Ensure that the insight is right.\\n- Ignore the \"index\" column.\\nTask 2: Please draw a Vega-Lite visualization to tell the insights based on the data table.\\nNotes for visualization generation:\\n- Your title should be less than 10 words.\\n- If you use the color channel , refer to the following color scheme: https://vega.github.io/vega/docs/schemes/\\n- Your visualization should have the right height and width.\\n- If the visualization is a line chart, it should include data points.\\n- If the focus of the data table presentation is on percentage information of one single column , then use a pie chart to\\npresent it. The percentage of each sector should be displayed on the corresponding sector via text annotation like\\nhttps://vega.github.io/vega-lite/examples/layer_arc_label.html.\\n- Use a single chart to visualize the data, and the index column should not be visualized.',\n",
       " 'https://vega.github.io/vega-lite/examples/layer_arc_label.html.\\n- Use a single chart to visualize the data, and the index column should not be visualized.\\nTask 3: Please write a streamlined narration for the insights.\\nNotes for narration generation:\\n- Writing narration in the tone of describing the visualization instead of describing the data table.\\n- Your logic should be compelling , linking insights into a story, and avoiding enumerating insights.\\n- Avoid using additional explanations. Avoid speculating on conclusions and redundant explanations beyond the data.\\n- Content should be streamlined.\\n- Ignore the \"index\" column.\\nThe final output JSON format is:\\n{\\n\"Insights\": [A],\\n\"Visualization\": [B],\\n\"Visualization_Type\":[C]\\n\"Narration\": [D]\\n}\\nwhere [A] is the listed insights in Task 1,\\n[B] is the visualization specification generated in Task 2, [C] is the\\nvisualization type, which is one of the values of [bar, scatter , pie, line], and [D] is the narration text for the\\ninsights in Task 3.',\n",
       " 'visualization type, which is one of the values of [bar, scatter , pie, line], and [D] is the narration text for the\\ninsights in Task 3.\\n7\\n© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization\\nconference. The final version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/\\nListing 3: The prompt of the agent acting as a designer\\nYou are a data video designer. You have a static visualization and corresponding insightful narration text at hand, as\\nwell as the original data table. If necessary , embellish the visualization with corresponding annotations (e.g.,\\narrow, text, circle, etc.) to tell the story more vividly in the narration text. Please think step by step.\\nThe visualization is: {{visualization}}\\nThe insightful narration is: {{narration}}\\nThe data table is: {{table}}\\nTask 1: You want some animation to appear on the visual elements during the audio narration of the video to aid in\\ntelling the data story.',\n",
       " 'The data table is: {{table}}\\nTask 1: You want some animation to appear on the visual elements during the audio narration of the video to aid in\\ntelling the data story.\\nConsider the narration as a timeline for the video. Insert animations inside the narration\\ntext where you feel they are needed. The corresponding animation will be triggered when the video reaches the\\ncorresponding narration segment and ends at the end of the narration segment.\\nNotes for animation generation:\\n- The output JSON format is:\\n[\\n{\\n\"animation\": animation type,\\n\"narration\": narration segment ,\\n\"target\": the visual elements that the animation applies to,\\n\"index\": a list of related data table rows index. If empty, output [],\\n\"explanation\": the explanation of why the animation needs to be added\\n}\\n]\\n- One sentence in the narration can correspond to multiple animations.\\n- Output only the part marked with animation.\\n- Narration text cannot be modified.',\n",
       " '}\\n]\\n- One sentence in the narration can correspond to multiple animations.\\n- Output only the part marked with animation.\\n- Narration text cannot be modified.\\n- There are three types of animations: entrance , emphasis , and exit.\\n- Entrance animations include [Axes-fade-in, Bar-grow-in, Line-wipe-in, Pie-wheel-in, Pie-wheel-in-and-legend -fly-in,\\nScatter -fade-in, Bar-grow-and-legend-fade-in, Line-wipe-and-legend -fade-in, Fade-in, Float-in, Fly-in, Zoom-in].\\n- Emphasis animations include [Bar-bounce, Zoom-in-then-zoom-out, Shine-in-a-short-duration , Highlight -one-and-fade-\\nothers].\\n- Exit animations include [Fade-out].\\n- [Axes fade in] animations can only be used at the beginning (first sentence) of a whole narration.\\n- Visual elements that have an entrance animation effect applied will not appear on the canvas until the animation is\\ntriggered. Elements that have an exit animation effect applied will disappear from the canvas after the animation.',\n",
       " 'triggered. Elements that have an exit animation effect applied will disappear from the canvas after the animation.\\nElements that do not have any animation effect applied will appear on the canvas by default.\\n- Visual elements can only be emphasized or disappear after they appear on the canvas , and elements cannot be emphasized\\nafter they disappear.\\nTask 2: Visualization embellishment generation\\nNotes for generation:\\n- Add annotation only when you think you need to, or export the original visualization if you do not feel the need to\\nadd it.\\n- Tag the narration text in the format of:\\n[\\n{\\n\"type\": a list of annotation types, which is one or a set from [mark label, circle , text, rule, trend line, arrow],\\n\"description\": annotation description and explanation ,\\n\"index\": a list of related data table rows index, If empty, output [],\\n\"nar\": narration segment\\n}\\n]\\n- The tagged annotations must correspond to the annotations in the visualization.',\n",
       " '\"index\": a list of related data table rows index, If empty, output [],\\n\"nar\": narration segment\\n}\\n]\\n- The tagged annotations must correspond to the annotations in the visualization.\\n- Only output the narration segments marked with annotation. If the value of key \"type\" is [], do not output the item.\\n- If the annotated vega-lite specification has a key \"layer\", then all \"mark\" and \"encoding\" keys should be inside the\\nlist value of the key \"layer\".\\n- The annotations should not be complex.\\n- The annotation must correspond to the narration segment. The annotation will appear when the video reaches the\\ncorresponding narration segment.\\n- The text annotation should be short (e.g., fewer than 6 words).\\n- Please output the complete bug-free vega-lite specification.\\nThe final output JSON format is:\\n{\\n\"Annotated_Visualization\": [A],\\n\"Annotated_Narration_for_Animation\": [B],\\n\"Annotated_Narration_for_Annotation\": [C]\\n}',\n",
       " 'The final output JSON format is:\\n{\\n\"Annotated_Visualization\": [A],\\n\"Annotated_Narration_for_Animation\": [B],\\n\"Annotated_Narration_for_Annotation\": [C]\\n}\\nwhere [A] is the generated annotated Vega-Lite specification , [B] is the tagged narration text for animation , and [C] is\\nthe tagged narration text for annotation.\\n8']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks = get_text_chunks(pdf_text)\n",
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(text_chunks):\n",
    "    #embeddings = OpenAIEmbeddings()\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "    # embeddings = HuggingFaceEmbeddings(model_name=\"nvidia/NV-Embed-v2\")\n",
    "    return FAISS.from_texts(text_chunks, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x13ec19f70>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = get_vector_store(text_chunks)\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(vector_store):\n",
    "    faiss_index = vector_store.index\n",
    "    num_vectors = faiss_index.ntotal\n",
    "    return [faiss_index.reconstruct(i) for i in range(num_vectors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.066182</td>\n",
       "      <td>-0.021010</td>\n",
       "      <td>-0.008742</td>\n",
       "      <td>-0.014602</td>\n",
       "      <td>-0.023218</td>\n",
       "      <td>0.008930</td>\n",
       "      <td>-0.051568</td>\n",
       "      <td>0.013058</td>\n",
       "      <td>-0.022419</td>\n",
       "      <td>0.015588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022173</td>\n",
       "      <td>-0.008046</td>\n",
       "      <td>-0.010913</td>\n",
       "      <td>0.023401</td>\n",
       "      <td>-0.019495</td>\n",
       "      <td>-0.030258</td>\n",
       "      <td>0.038941</td>\n",
       "      <td>-0.001185</td>\n",
       "      <td>-0.031221</td>\n",
       "      <td>0.011723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.045843</td>\n",
       "      <td>-0.050158</td>\n",
       "      <td>-0.023588</td>\n",
       "      <td>0.006357</td>\n",
       "      <td>-0.034032</td>\n",
       "      <td>0.021442</td>\n",
       "      <td>-0.016376</td>\n",
       "      <td>0.021591</td>\n",
       "      <td>-0.007568</td>\n",
       "      <td>0.009252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027476</td>\n",
       "      <td>-0.002983</td>\n",
       "      <td>-0.017811</td>\n",
       "      <td>0.013417</td>\n",
       "      <td>-0.023275</td>\n",
       "      <td>-0.035028</td>\n",
       "      <td>0.013760</td>\n",
       "      <td>-0.004402</td>\n",
       "      <td>-0.008546</td>\n",
       "      <td>-0.024951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.045072</td>\n",
       "      <td>-0.016056</td>\n",
       "      <td>-0.036921</td>\n",
       "      <td>0.017648</td>\n",
       "      <td>-0.045985</td>\n",
       "      <td>0.021246</td>\n",
       "      <td>-0.045944</td>\n",
       "      <td>0.028198</td>\n",
       "      <td>-0.019140</td>\n",
       "      <td>-0.000546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019373</td>\n",
       "      <td>0.006401</td>\n",
       "      <td>0.004539</td>\n",
       "      <td>-0.002458</td>\n",
       "      <td>-0.016572</td>\n",
       "      <td>-0.015448</td>\n",
       "      <td>0.038689</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>-0.017255</td>\n",
       "      <td>-0.017569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.028486</td>\n",
       "      <td>0.015219</td>\n",
       "      <td>-0.002378</td>\n",
       "      <td>-0.019613</td>\n",
       "      <td>-0.040951</td>\n",
       "      <td>-0.030104</td>\n",
       "      <td>-0.000281</td>\n",
       "      <td>-0.004606</td>\n",
       "      <td>-0.002633</td>\n",
       "      <td>0.007565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>0.009628</td>\n",
       "      <td>0.010895</td>\n",
       "      <td>0.020558</td>\n",
       "      <td>-0.050434</td>\n",
       "      <td>-0.035305</td>\n",
       "      <td>0.042794</td>\n",
       "      <td>-0.006519</td>\n",
       "      <td>-0.022085</td>\n",
       "      <td>-0.035296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.050036</td>\n",
       "      <td>-0.013621</td>\n",
       "      <td>-0.000630</td>\n",
       "      <td>-0.012360</td>\n",
       "      <td>-0.035588</td>\n",
       "      <td>-0.002847</td>\n",
       "      <td>-0.016386</td>\n",
       "      <td>0.019581</td>\n",
       "      <td>-0.009440</td>\n",
       "      <td>0.025057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.014324</td>\n",
       "      <td>-0.001540</td>\n",
       "      <td>0.018827</td>\n",
       "      <td>-0.008673</td>\n",
       "      <td>-0.029025</td>\n",
       "      <td>0.014733</td>\n",
       "      <td>-0.003233</td>\n",
       "      <td>-0.037107</td>\n",
       "      <td>-0.010664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0 -0.066182 -0.021010 -0.008742 -0.014602 -0.023218  0.008930 -0.051568   \n",
       "1 -0.045843 -0.050158 -0.023588  0.006357 -0.034032  0.021442 -0.016376   \n",
       "2 -0.045072 -0.016056 -0.036921  0.017648 -0.045985  0.021246 -0.045944   \n",
       "3 -0.028486  0.015219 -0.002378 -0.019613 -0.040951 -0.030104 -0.000281   \n",
       "4 -0.050036 -0.013621 -0.000630 -0.012360 -0.035588 -0.002847 -0.016386   \n",
       "\n",
       "       7         8         9     ...      1014      1015      1016      1017  \\\n",
       "0  0.013058 -0.022419  0.015588  ...  0.022173 -0.008046 -0.010913  0.023401   \n",
       "1  0.021591 -0.007568  0.009252  ...  0.027476 -0.002983 -0.017811  0.013417   \n",
       "2  0.028198 -0.019140 -0.000546  ...  0.019373  0.006401  0.004539 -0.002458   \n",
       "3 -0.004606 -0.002633  0.007565  ...  0.001240  0.009628  0.010895  0.020558   \n",
       "4  0.019581 -0.009440  0.025057  ...  0.001720  0.014324 -0.001540  0.018827   \n",
       "\n",
       "       1018      1019      1020      1021      1022      1023  \n",
       "0 -0.019495 -0.030258  0.038941 -0.001185 -0.031221  0.011723  \n",
       "1 -0.023275 -0.035028  0.013760 -0.004402 -0.008546 -0.024951  \n",
       "2 -0.016572 -0.015448  0.038689  0.012200 -0.017255 -0.017569  \n",
       "3 -0.050434 -0.035305  0.042794 -0.006519 -0.022085 -0.035296  \n",
       "4 -0.008673 -0.029025  0.014733 -0.003233 -0.037107 -0.010664  \n",
       "\n",
       "[5 rows x 1024 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame = pd.DataFrame(get_vectors(vector_store))\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chain(vectorstore):\n",
    "    llm = ChatOpenAI()\n",
    "    # llm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512})\n",
    "\n",
    "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        memory=memory\n",
    "    )\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationalRetrievalChain(memory=ConversationBufferMemory(return_messages=True, memory_key='chat_history'), combine_docs_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x339b60830>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x39c584ec0>, root_client=<openai.OpenAI object at 0x339b61c10>, root_async_client=<openai.AsyncOpenAI object at 0x339b60800>, openai_api_key=SecretStr('**********'), openai_proxy='')), document_variable_name='context'), question_generator=LLMChain(prompt=PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x339b60830>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x39c584ec0>, root_client=<openai.OpenAI object at 0x339b61c10>, root_async_client=<openai.AsyncOpenAI object at 0x339b60800>, openai_api_key=SecretStr('**********'), openai_proxy='')), retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x13ec19f70>))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = get_chain(vector_store) \n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the story about?',\n",
       " 'chat_history': [HumanMessage(content='What is the story about?'),\n",
       "  AIMessage(content='The story is about leveraging large language models (LLMs) to create autonomous agents that can streamline the data storytelling workflow. These agents, powered by LLMs, help in perceiving environments, making decisions, and taking actions to automate tasks related to data analysis and information communication. The goal is to enhance the efficiency of data analysis and storytelling by utilizing the capabilities of LLMs in various tasks like data analysis, document generation, and visualization creation.')],\n",
       " 'answer': 'The story is about leveraging large language models (LLMs) to create autonomous agents that can streamline the data storytelling workflow. These agents, powered by LLMs, help in perceiving environments, making decisions, and taking actions to automate tasks related to data analysis and information communication. The goal is to enhance the efficiency of data analysis and storytelling by utilizing the capabilities of LLMs in various tasks like data analysis, document generation, and visualization creation.'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is the story about?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
